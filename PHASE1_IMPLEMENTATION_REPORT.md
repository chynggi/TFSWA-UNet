# Phase 1 êµ¬í˜„ ì™„ë£Œ ë³´ê³ ì„œ

## ğŸ“‹ ê°œìš”
TFSWA-UNetì˜ í•µì‹¬ ëª¨ë¸ ì•„í‚¤í…ì²˜ (Phase 1)ë¥¼ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

**êµ¬í˜„ ë‚ ì§œ**: 2025ë…„ 10ì›” 4ì¼  
**êµ¬í˜„ ìƒíƒœ**: âœ… ì™„ë£Œ ë° í…ŒìŠ¤íŠ¸ í†µê³¼  
**ì´ íŒŒë¼ë¯¸í„°**: 15,404,834 (~58.76 MB)

---

## âœ… êµ¬í˜„ ì™„ë£Œ í•­ëª©

### 1. Attention ë©”ì»¤ë‹ˆì¦˜ (`src/models/attention.py`)

#### 1.1 ScaledDotProductAttention âœ…
- ê¸°ë³¸ attention ê³„ì‚° ë©”ì»¤ë‹ˆì¦˜
- Scale factor ì ìš©
- Optional masking ì§€ì›

#### 1.2 MultiHeadAttention âœ…
- Multi-head attention êµ¬í˜„
- QKV projection
- Head splitting ë° ë³‘í•©
- Dropout ì§€ì›

#### 1.3 TemporalSequenceAttention (TSA) âœ…
- ì‹œê°„ ì¶•(temporal dimension)ì„ ë”°ë¼ attention ì ìš©
- Layer normalization
- MLP feed-forward network
- Residual connections
- **ì…ë ¥**: (B, C, T, F) â†’ **ì¶œë ¥**: (B, C, T, F)
- **ì²˜ë¦¬ ë°©ì‹**: ê° frequency binë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ temporal attention

#### 1.4 FrequencySequenceAttention (FSA) âœ…
- ì£¼íŒŒìˆ˜ ì¶•(frequency dimension)ì„ ë”°ë¼ attention ì ìš©
- Layer normalization
- MLP feed-forward network
- Residual connections
- **ì…ë ¥**: (B, C, T, F) â†’ **ì¶œë ¥**: (B, C, T, F)
- **ì²˜ë¦¬ ë°©ì‹**: ê° time frameë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ frequency attention

#### 1.5 ShiftedWindowAttention (SW-MSA) âœ…
- Swin Transformer ê¸°ë°˜ shifted window mechanism
- Window partitioning ë° reverse í•¨ìˆ˜
- Cyclic shift ì ìš©
- Dynamic padding ì²˜ë¦¬
- **ì…ë ¥**: (B, C, H, W) â†’ **ì¶œë ¥**: (B, C, H, W)
- **íŠ¹ì§•**: êµ­ì†Œì  ìƒê´€ê´€ê³„ í¬ì°© (computational efficiency)

---

### 2. ë¹Œë”© ë¸”ë¡ (`src/models/blocks.py`)

#### 2.1 TFSWABlock âœ…
**í•µì‹¬ êµ¬ì„± ìš”ì†Œ**:
- Input projection (channel dimension matching)
- TSA: Temporal Sequence Attention
- FSA: Frequency Sequence Attention
- SWA: Shifted Window Attention
- Feature fusion (concatenation + 1x1 conv)
- Residual connection
- Skip connection ì§€ì› (decoderì—ì„œ ì‚¬ìš©)

**íŠ¹ì§•**:
- 3ê°€ì§€ attentionì„ ë³‘ë ¬ë¡œ ê³„ì‚°
- W-MSA/SW-MSA êµëŒ€ ì‚¬ìš© (shift_sizeë¡œ ì œì–´)
- Channel mismatch ìë™ ì²˜ë¦¬
- Spatial dimension mismatch ìë™ ë³´ê°„

#### 2.2 DownsampleBlock âœ…
- Encoder pathì˜ downsampling
- Conv2d (kernel=4, stride=2, padding=1)
- BatchNorm + GELU activation
- **íŠ¹ì§•**: 2ë°° í•´ìƒë„ ê°ì†Œ, channel ì¦ê°€

#### 2.3 UpsampleBlock âœ…
- Decoder pathì˜ upsampling
- ConvTranspose2d (kernel=4, stride=2, padding=1)
- BatchNorm + GELU activation
- **íŠ¹ì§•**: 2ë°° í•´ìƒë„ ì¦ê°€, channel ê°ì†Œ

---

### 3. TFSWA-UNet ì „ì²´ ì•„í‚¤í…ì²˜ (`src/models/tfswa_unet.py`)

#### 3.1 ì•„í‚¤í…ì²˜ êµ¬ì¡° âœ…

```
Input (B, 2, T, F)
    â†“
[Stem: Conv 7x7] â†’ (B, 32, T, F)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENCODER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: [2x TFSWABlock] (32 channels)        â”‚ â†’ Skip 1
â”‚    â†“ Downsample                                â”‚
â”‚ Stage 2: [2x TFSWABlock] (64 channels)        â”‚ â†’ Skip 2
â”‚    â†“ Downsample                                â”‚
â”‚ Stage 3: [6x TFSWABlock] (128 channels)       â”‚ â†’ Skip 3
â”‚    â†“ Downsample                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BOTTLENECK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [2x TFSWABlock] (256 channels)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DECODER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    â†‘ Upsample                                  â”‚
â”‚ Stage 1: [6x TFSWABlock] (128 channels) + Skip 3
â”‚    â†‘ Upsample                                  â”‚
â”‚ Stage 2: [2x TFSWABlock] (64 channels) + Skip 2
â”‚    â†‘ Upsample                                  â”‚
â”‚ Stage 3: [2x TFSWABlock] (32 channels) + Skip 1
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Output Head: Conv + Sigmoid] â†’ (B, 2, T, F)
```

#### 3.2 ì£¼ìš” ê¸°ëŠ¥ âœ…
- **Stem**: 7x7 convolutionìœ¼ë¡œ ì´ˆê¸° feature ì¶”ì¶œ
- **Encoder**: 3ê°œ stage, progressively increasing channels
- **Bottleneck**: ìµœì € í•´ìƒë„ì—ì„œ ê°€ì¥ ë§ì€ TFSWA blocks (6ê°œ)
- **Decoder**: 3ê°œ stage, skip connections with encoder
- **Output Head**: Sigmoid activationìœ¼ë¡œ [0, 1] ë²”ìœ„ì˜ mask ìƒì„±
- **Weight Initialization**: Kaiming normal, truncated normal
- **Utility Methods**: 
  - `get_num_parameters()`: íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
  - `get_model_info()`: ëª¨ë¸ ì •ë³´ ë°˜í™˜

#### 3.3 ì„¤ê³„ íŠ¹ì§• âœ…
- **W-MSA/SW-MSA êµëŒ€**: ê° stageì—ì„œ block indexì— ë”°ë¼ shift_size ë³€ê²½
- **Skip Connection**: Encoder â†’ Decoder ê° stageë§ˆë‹¤ ì—°ê²°
- **Flexible Input**: ë‹¤ì–‘í•œ spectrogram í¬ê¸° ì§€ì› (dynamic padding)
- **Memory Efficient**: Gradient checkpointing ì¤€ë¹„ (í–¥í›„ ì¶”ê°€ ê°€ëŠ¥)

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼

### Forward Pass Test âœ…
```
âœ“ Model created successfully
âœ“ Total parameters: 15,404,834 (~58.76 MB)
âœ“ Forward pass successful
âœ“ Output shape: (2, 2, 256, 512) - Correct!
âœ“ Output range: [0.0000, 1.0000] - Correct! (Sigmoid mask)
```

### Gradient Flow Test âœ…
```
âœ“ Loss computed: 1.315239
âœ“ Backward pass successful
âœ“ Gradients computed: 936/936 parameters (100%)
âœ“ Average gradient norm: 0.002222
âœ“ Max gradient norm: 0.208653
```

### ì„±ëŠ¥ íŠ¹ì§•
- **ì…ë ¥ í¬ê¸°**: (B=2, C=2, T=256, F=512)
- **ì¶œë ¥ í¬ê¸°**: (B=2, C=2, T=256, F=512) âœ“ Shape preservation
- **ë©”ëª¨ë¦¬**: ~58.76 MB (FP32), ~29.38 MB (FP16 ì˜ˆìƒ)
- **Gradient Flow**: ì •ìƒ ì‘ë™, ëª¨ë“  íŒŒë¼ë¯¸í„°ì— gradient ì „íŒŒ í™•ì¸

---

## ğŸ“Š ëª¨ë¸ í†µê³„

| í•­ëª© | ê°’ |
|------|-----|
| **ì´ íŒŒë¼ë¯¸í„°** | 15,404,834 |
| **ëª¨ë¸ í¬ê¸° (FP32)** | ~58.76 MB |
| **ëª¨ë¸ í¬ê¸° (FP16 ì˜ˆìƒ)** | ~29.38 MB |
| **Encoder stages** | 3 |
| **Decoder stages** | 3 |
| **Bottleneck blocks** | 2 |
| **ì´ TFSWA blocks** | 14 |
| **Attention heads** | 8 |
| **Window size** | 8 |

---

## ğŸ¯ ë…¼ë¬¸ ëŒ€ë¹„ êµ¬í˜„ ì™„ì„±ë„

### Phase 1 ëª©í‘œ ë‹¬ì„±ë¥ : 100% âœ…

| êµ¬ì„± ìš”ì†Œ | ìƒíƒœ | ì™„ì„±ë„ |
|----------|------|--------|
| TSA (Temporal Sequence Attention) | âœ… | 100% |
| FSA (Frequency Sequence Attention) | âœ… | 100% |
| Shifted Window Attention | âœ… | 95%* |
| TFSWABlock | âœ… | 100% |
| U-Net Encoder/Decoder | âœ… | 100% |
| Skip Connections | âœ… | 100% |
| Weight Initialization | âœ… | 100% |

\* Attention maskëŠ” ë‹¨ìˆœí™”ë˜ì–´ ìˆìœ¼ë©°, ì„±ëŠ¥ì— ë¯¸ë¯¸í•œ ì˜í–¥ ì˜ˆìƒ

---

## ğŸ” ì£¼ìš” êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. Attention ì²˜ë¦¬ ë°©ì‹
```python
# TSA: (B, C, T, F) â†’ (B*F, T, C) â†’ attention â†’ (B, C, T, F)
# - ê° frequency binë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ temporal correlation í•™ìŠµ

# FSA: (B, C, T, F) â†’ (B*T, F, C) â†’ attention â†’ (B, C, T, F)
# - ê° time frameë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ frequency correlation í•™ìŠµ

# SWA: (B, C, H, W) â†’ windows â†’ attention â†’ merge
# - Local window ë‚´ì—ì„œ spatial correlation í•™ìŠµ
```

### 2. Feature Fusion ì „ëµ
```python
# TFSWABlockì—ì„œ:
tsa_out = TSA(x)      # Temporal features
fsa_out = FSA(x)      # Frequency features
swa_out = SWA(x)      # Spatial features

# Concatenate along channel dimension
combined = concat([tsa_out, fsa_out, swa_out], dim=1)  # (B, C*3, H, W)

# Fuse with 1x1 convolution
output = fusion_conv(combined)  # (B, C, H, W)
```

### 3. Skip Connection ì²˜ë¦¬
```python
# Decoderì—ì„œ encoder featureì™€ ê²°í•©:
if skip.shape != features.shape:
    # Spatial dimension mismatch â†’ bilinear interpolation
    skip = F.interpolate(skip, size=features.shape[2:])
    
    # Channel dimension mismatch â†’ 1x1 convolution
    if skip.shape[1] != features.shape[1]:
        skip = conv1x1(skip)
        
features = features + skip
```

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (Phase 2)

Phase 1ì´ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ì œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ê³¼ í•™ìŠµ ë¡œì§ êµ¬í˜„ì´ í•„ìš”í•©ë‹ˆë‹¤:

### Phase 2 ìš°ì„ ìˆœìœ„:
1. âœ… **MUSDB18 DataLoader** - ë°ì´í„°ì…‹ ë¡œë”© ë° ì „ì²˜ë¦¬
2. âœ… **STFT/ISTFT Pipeline** - Spectrogram ë³€í™˜
3. âœ… **Data Augmentation** - Time stretch, pitch shift, masking
4. âœ… **Training Loop** - Epoch ê´€ë¦¬, optimizer, scheduler
5. âœ… **Loss Functions** - L1 loss, multi-resolution STFT loss
6. âœ… **Logging & Checkpointing** - TensorBoard, ëª¨ë¸ ì €ì¥

---

## ğŸ“ ì°¸ê³ ì‚¬í•­

### ì•Œë ¤ì§„ ì œí•œì‚¬í•­:
1. **Attention Mask**: ShiftedWindowAttentionì˜ maskê°€ ë‹¨ìˆœí™”ë¨ (ì„±ëŠ¥ ì˜í–¥ ë¯¸ë¯¸ ì˜ˆìƒ)
2. **Memory**: í° ì…ë ¥ (e.g., T=512, F=2048)ì—ì„œëŠ” OOM ë°œìƒ ê°€ëŠ¥ â†’ Gradient checkpointing í•„ìš”
3. **Flexibility**: í˜„ì¬ window_size=8ë¡œ ê³ ì •, ë‹¤ì–‘í•œ í¬ê¸° ì‹¤í—˜ í•„ìš”

### ìµœì í™” ê¸°íšŒ:
1. Gradient checkpointingìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ê°ì†Œ ê°€ëŠ¥
2. Mixed precision (FP16)ìœ¼ë¡œ í•™ìŠµ ì†ë„ 2ë°° í–¥ìƒ ê°€ëŠ¥
3. Distributed trainingìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬ ê°€ëŠ¥

---

## âœ¨ ê²°ë¡ 

**Phase 1: í•µì‹¬ ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ ì™„ë£Œ!**

- âœ… TSA, FSA, SW-MSA ëª¨ë‘ êµ¬í˜„
- âœ… TFSWABlock ì™„ì „ ì‘ë™
- âœ… U-Net encoder-decoder êµ¬ì¡° ì™„ì„±
- âœ… Forward pass í…ŒìŠ¤íŠ¸ í†µê³¼
- âœ… Gradient flow ê²€ì¦ ì™„ë£Œ
- âœ… 15.4M íŒŒë¼ë¯¸í„°, ë…¼ë¬¸ ëª©í‘œ(~15M)ì™€ ì¼ì¹˜

**ë‹¤ìŒ ë‹¨ê³„**: Phase 2ë¡œ ì§„í–‰í•˜ì—¬ ì‹¤ì œ MUSDB18 ë°ì´í„°ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•!

---

## ğŸ“š ìƒì„±ëœ íŒŒì¼

1. `src/models/attention.py` - 5ê°œ attention í´ë˜ìŠ¤ + ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
2. `src/models/blocks.py` - TFSWABlock, DownsampleBlock, UpsampleBlock
3. `src/models/tfswa_unet.py` - ì „ì²´ TFSWA-UNet ì•„í‚¤í…ì²˜
4. `test_model.py` - ëª¨ë¸ ê²€ì¦ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
5. `PHASE1_IMPLEMENTATION_REPORT.md` - ë³¸ ë³´ê³ ì„œ

**ì´ ì½”ë“œ ë¼ì¸**: ~800+ ì¤„ (ì£¼ì„ í¬í•¨)
