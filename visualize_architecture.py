"""Visualize TFSWA-UNet architecture structure."""
import torch
from src.models.tfswa_unet import TFSWAUNet


def print_architecture():
    """Print detailed architecture structure."""
    
    print("=" * 80)
    print(" " * 25 + "TFSWA-UNet Architecture")
    print("=" * 80)
    
    config = {
        "in_channels": 2,
        "out_channels": 2,
        "depths": [2, 2, 6, 2],
        "dims": [32, 64, 128, 256],
        "window_size": 8,
        "shift_size": 4,
        "num_heads": 8,
    }
    
    model = TFSWAUNet(**config)
    
    print("\nğŸ“Š Model Configuration:")
    print("-" * 80)
    for key, value in config.items():
        print(f"  {key:20s}: {value}")
    
    info = model.get_model_info()
    print(f"\n  {'num_parameters':20s}: {info['num_parameters']:,}")
    print(f"  {'model_size_fp32':20s}: ~{info['num_parameters'] * 4 / 1024 / 1024:.2f} MB")
    print(f"  {'model_size_fp16':20s}: ~{info['num_parameters'] * 2 / 1024 / 1024:.2f} MB")
    
    print("\n" + "=" * 80)
    print(" " * 30 + "Architecture Flow")
    print("=" * 80)
    
    # Trace through architecture
    B, C_in, T, F = 2, 2, 256, 512
    print(f"\nğŸ”¹ INPUT: ({B}, {C_in}, {T}, {F})")
    print("   â†“")
    
    # Stem
    print(f"â”Œâ”€ STEM (Conv 7x7 + BatchNorm + GELU)")
    print(f"â””â”€ â†’ ({B}, {config['dims'][0]}, {T}, {F})")
    print("   â†“")
    
    # Encoder
    print("â”Œ" + "â”€" * 78 + "â”")
    print("â”‚" + " " * 32 + "ENCODER" + " " * 40 + "â”‚")
    print("â”œ" + "â”€" * 78 + "â”¤")
    
    current_T, current_F = T, F
    for i, (depth, dim) in enumerate(zip(config['depths'][:-1], config['dims'][:-1])):
        print(f"â”‚ Stage {i+1}:")
        print(f"â”‚   [{depth}x TFSWABlock] (dim={dim}, heads={config['num_heads']})")
        print(f"â”‚   - TSA: Temporal attention along {current_T} time frames")
        print(f"â”‚   - FSA: Frequency attention along {current_F} frequency bins")
        print(f"â”‚   - SWA: Shifted window attention (window={config['window_size']})")
        print(f"â”‚   Shape: ({B}, {dim}, {current_T}, {current_F})")
        print(f"â”‚   â†’ Skip Connection {i+1}")
        
        if i < len(config['depths']) - 2:
            current_T //= 2
            current_F //= 2
            print(f"â”‚   â†“ DOWNSAMPLE (stride=2)")
            print(f"â”‚   â†’ ({B}, {config['dims'][i+1]}, {current_T}, {current_F})")
        
        if i < len(config['depths']) - 2:
            print("â”‚" + " " * 78 + "â”‚")
    
    print("â””" + "â”€" * 78 + "â”˜")
    print("   â†“")
    
    # Bottleneck
    print("â”Œ" + "â”€" * 78 + "â”")
    print("â”‚" + " " * 31 + "BOTTLENECK" + " " * 37 + "â”‚")
    print("â”œ" + "â”€" * 78 + "â”¤")
    print(f"â”‚ [{config['depths'][-1]}x TFSWABlock] (dim={config['dims'][-1]}, heads={config['num_heads']})")
    print(f"â”‚   Shape: ({B}, {config['dims'][-1]}, {current_T}, {current_F})")
    print("â””" + "â”€" * 78 + "â”˜")
    print("   â†“")
    
    # Decoder
    print("â”Œ" + "â”€" * 78 + "â”")
    print("â”‚" + " " * 32 + "DECODER" + " " * 41 + "â”‚")
    print("â”œ" + "â”€" * 78 + "â”¤")
    
    for i in range(len(config['depths']) - 2, -1, -1):
        current_T *= 2
        current_F *= 2
        depth = config['depths'][i]
        dim = config['dims'][i]
        
        print(f"â”‚   â†‘ UPSAMPLE (stride=2)")
        print(f"â”‚   â†’ ({B}, {dim}, {current_T}, {current_F})")
        print(f"â”‚ Stage {i+1}:")
        print(f"â”‚   [{depth}x TFSWABlock] (dim={dim}, heads={config['num_heads']})")
        print(f"â”‚   + Skip Connection {i+1} from Encoder")
        print(f"â”‚   Shape: ({B}, {dim}, {current_T}, {current_F})")
        
        if i > 0:
            print("â”‚" + " " * 78 + "â”‚")
    
    print("â””" + "â”€" * 78 + "â”˜")
    print("   â†“")
    
    # Output
    print(f"â”Œâ”€ OUTPUT HEAD (Conv + Sigmoid)")
    print(f"â””â”€ â†’ ({B}, {config['out_channels']}, {T}, {F})")
    print()
    print(f"ğŸ”¹ OUTPUT: Separation masks in [0, 1] range")
    
    print("\n" + "=" * 80)
    print(" " * 28 + "Module Breakdown")
    print("=" * 80)
    
    # Count parameters by module
    def count_parameters(module):
        return sum(p.numel() for p in module.parameters())
    
    print(f"\n{'Module':<30s} {'Parameters':>15s} {'% of Total':>15s}")
    print("-" * 80)
    
    total_params = model.get_num_parameters()
    
    stem_params = count_parameters(model.stem)
    print(f"{'Stem':<30s} {stem_params:>15,d} {stem_params/total_params*100:>14.2f}%")
    
    encoder_params = sum(count_parameters(stage) for stage in model.encoder_stages)
    print(f"{'Encoder (all stages)':<30s} {encoder_params:>15,d} {encoder_params/total_params*100:>14.2f}%")
    
    downsample_params = sum(count_parameters(layer) for layer in model.downsample_layers)
    print(f"{'Downsample layers':<30s} {downsample_params:>15,d} {downsample_params/total_params*100:>14.2f}%")
    
    bottleneck_params = count_parameters(model.bottleneck)
    print(f"{'Bottleneck':<30s} {bottleneck_params:>15,d} {bottleneck_params/total_params*100:>14.2f}%")
    
    upsample_params = sum(count_parameters(layer) for layer in model.upsample_layers)
    print(f"{'Upsample layers':<30s} {upsample_params:>15,d} {upsample_params/total_params*100:>14.2f}%")
    
    decoder_params = sum(count_parameters(stage) for stage in model.decoder_stages)
    print(f"{'Decoder (all stages)':<30s} {decoder_params:>15,d} {decoder_params/total_params*100:>14.2f}%")
    
    output_params = count_parameters(model.output_head)
    print(f"{'Output head':<30s} {output_params:>15,d} {output_params/total_params*100:>14.2f}%")
    
    print("-" * 80)
    print(f"{'TOTAL':<30s} {total_params:>15,d} {'100.00%':>15s}")
    
    print("\n" + "=" * 80)
    print(" " * 25 + "Attention Mechanisms Detail")
    print("=" * 80)
    
    print("\n1ï¸âƒ£  TSA (Temporal Sequence Attention)")
    print("   â€¢ Processes temporal dimension (time frames)")
    print("   â€¢ Each frequency bin independently")
    print("   â€¢ Captures long-range temporal dependencies")
    print("   â€¢ Transform: (B, C, T, F) â†’ (B*F, T, C) â†’ attention â†’ (B, C, T, F)")
    
    print("\n2ï¸âƒ£  FSA (Frequency Sequence Attention)")
    print("   â€¢ Processes frequency dimension (frequency bins)")
    print("   â€¢ Each time frame independently")
    print("   â€¢ Captures frequency-domain relationships")
    print("   â€¢ Transform: (B, C, T, F) â†’ (B*T, F, C) â†’ attention â†’ (B, C, T, F)")
    
    print("\n3ï¸âƒ£  SWA (Shifted Window Attention)")
    print("   â€¢ Processes 2D spatial structure")
    print(f"   â€¢ Window size: {config['window_size']}x{config['window_size']}")
    print(f"   â€¢ Shift size: {config['shift_size']} (for SW-MSA)")
    print("   â€¢ Alternates between W-MSA (shift=0) and SW-MSA (shift>0)")
    print("   â€¢ Captures local spatial correlations efficiently")
    
    print("\nğŸ’¡ Feature Fusion Strategy:")
    print("   TSA_out â”€â”")
    print("   FSA_out â”€â”¼â”€â†’ Concat â†’ 1x1 Conv â†’ Fused Features")
    print("   SWA_out â”€â”˜")
    
    print("\n" + "=" * 80)
    print("âœ… Phase 1 Implementation Complete!")
    print("=" * 80)


if __name__ == "__main__":
    print_architecture()
